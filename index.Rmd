---
title: "The evolution of Rock"
author: "Qi Draaisma"
date: "10-2-2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: simplex
---

``` {r echo=FALSE, message=FALSE, results=FALSE, warnings=FALSE}
library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)

spotifyr::get_spotify_access_token() 

```

```{r}
RockNRoll <- get_playlist_audio_features("thesoundofspotify", "2ii0K7mNdB89XgcOCUM64t") %>%
  slice(1:50) %>%
  add_audio_analysis()
RockClassics <- get_playlist_audio_features("thesoundofspotify", "37i9dQZF1DWXRqgorJj26U") %>%
  slice(1:50) %>%
  add_audio_analysis()
RockSubGenres <- get_playlist_audio_features("thesoundofspotify", "5dZcUWhn9DP8oOWVQVYLq4") %>%
  slice(1:50) %>%
  add_audio_analysis()

Rock <- bind_rows(
  RockNRoll %>% mutate(era = "Rock 'n Roll (50s, 60s)"),
  RockClassics %>% mutate(era = "Rock Classics (70s, 80s)"),
  RockSubGenres %>% mutate(era = "Rock Subgenres (90s, 00s)")
)

RockMeans <- bind_rows(
  RockNRoll %>% summarise(
    mean_valence = mean(valence),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    era = "Rock 'n Roll (50s, 60s)"
  ),
  RockClassics %>% summarise(
    mean_valence = mean(valence),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    era = "Rock Classics (70s, 80s)"
  ),
  RockSubGenres %>% summarise(
    mean_valence = mean(valence),
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    era = "Rock Subgenres (90s, 00s)"
  ),
  )

```
Welcome to the Rock evolution
==================
Column {data-width=400}
-------------------------------------
```{r}
neckdf <- data.frame(
  xcoord = append(seq(1, 50),seq(0.5, 45)),
  ycoord = append(seq(1, 50), seq(0.5, 45)),
  s = append(append(rep(20, each=45), c(50, 40, 30, 20, 20)), rep(10, each=45))
)

bodydf <- data.frame(
  xcoord = append(append(append(append(append(append(seq(1, 20), seq(-1, 15)),
                  seq(-3, 12)),
                  seq(-5, 8)),
                  seq(-7, 4)),
                  seq(-9, 0)),
                  seq(-11, -4)
  ),
  ycoord = append(append(append(append(append(append(seq(1, 20), seq(1, 17)),
                  seq(1, 16)),
                  seq(1, 14)),
                  seq(1, 12)),
                  seq(1, 10)),
                  seq(1, 8)
  )
)
invbodydf <- data.frame(
  xcoord = bodydf$ycoord,
  ycoord = bodydf$xcoord
)

guitardf <- bind_rows(
  neckdf %>% mutate(parts = "Neck", color = "brown"),
  bodydf %>% mutate(parts = "Body", s = 50, color = "black"),
  invbodydf %>% mutate(parts = "Body", s = 50, color = "white")
)

ggplot(guitardf, aes(x = xcoord, y = ycoord, size = s, alpha = 0.75)) +
  geom_point() +
  ggtitle("Enjoy this guitarplot") +
  theme_minimal() +
  theme(
    legend.position = "none"
  )
```



Column {data-width=600}
-------------------------------------
### Course portfolio for Computational Musicology

For this portfolio, I will be looking at the evolution of the Rock genre. Specifically, from Rock 'n Roll into Rock music and eventually into more alternative Rock subgenres,
 like Grunge, Metal and alt Rock. For the Rock 'n Roll songs, the Spotify playlist "50's and 60's Rock n Roll" playlist has been chosen to process. There are 74 songs within 
 the playlist and includes classic Rock 'n Roll artists, like Elvis Presley, Chuck Berry, Little Richard and much more. The playlist for the Rock music will be the Rock Classics 
 playlist from Spotify itself. This playlist includes 145 songs of classic Rock songs and is very likely to be suited for this era of Rock music, because it contains a lot of 
 different styles of Rock within that era. Finally, for the alternative Rock subgenres a playlist named "90s Grunge/Rock/Punk-pop" is used with a total of 153 songs in the playlist. 
To even out the amount of songs used, this research will use 70 of the playlists songs in order to make sure we have the same amount of data throughout the era's of Rock music.

### Why Rock
 The decision to look at the evolution of Rock music came to me, because of my obsession with why people seem to know these classical Rock songs no matter the age of 
 the person. Classic Rock music seems to be something almost everyone can enjoy, albeit not all the subgenres. Within the alternative Rock category, there are bands like 
 The Offspring and Green Day, which are well known for their heavier "Punk" style Rock. Take for example "Basket Case" from Green Day, which has more than half a billion
 streams. This is not even accounting for the amount of times the song is listened to in any other way. Next to this Punk style, the genre still contains bands like U2 as well. 
 The style of U2 is completely different, but rooted from the same type of music. Rock.

 All in all, I expect the loudness of Rock music in it's evolution to rise, this because of the fact that in later stages, the guitars became more distorted. Next to that, I expect 
 that the valence, which measures the positivity of a song, will be lower in the alternative Rock category because of the fact that the songs are written in less of a "swing"
 than the early Rock 'n Roll days.

The Rocking research {.storyboard}
====================
### Comparing all the different era's in one big Rockplot


``` {r}
fullplot <- Rock %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
ggplot(
  aes(
    x = valence,
    y = danceability,
    color = mode,
    size = energy,
    label = track.name
    )
  ) +
  geom_point(alpha=0.4) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 1)) +
  facet_wrap(~era)

ggplotly(fullplot)

```

*** 
Here we see three plots. In these plots the different era's are shown with valence on the x-axis (This means how "happy" the song sounds), the danceability on the y-axis and the size depicts the energy of the song. Then only the color is left, which depicts whether the song is in a major or minor key.

A very interesting fact that can be concluded from the plots on the left is that it is apparent that the Rock genre got a lot more diverse as time went on. In the plot of Rock 'n Roll, note that the average danceability and valence are very high. In the period following directly after, the Rock Classics, the valence is a lot more spread out than it was before. The period after this, The Rock Subgenres, it is seen that the average danceability has gone down significantly.

Next to these findings, it is also apparent in the plots that in the Rock 'n Roll period, the overall energy was lower than the energy of later tracks. This can be noted by the fact that the left plot seems less crowded. This effect is however magnified by the fact that the Rock 'n Roll playlist had less songs than the other plots.

In the Appendix tab, you can find the plots on their own to increase the readibility of the plots in comparison to the three plots next to each other.






### Difference in Tempo, Duration and Loudness throughout the evolution

``` {r}
Erameans <- Rock %>%
  group_by(era) %>%
  summarise(mtemp = mean(tempo))

Rock %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = era,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_segment(aes(x = Erameans$mtemp[1], xend = Erameans$mtemp[1], y = 0, yend = 7, alpha = 0.2), color = "red") +
  geom_segment(aes(x = Erameans$mtemp[2], xend = Erameans$mtemp[2], y = 0, yend = 7, alpha = 0.2), color = "green") +
  geom_segment(aes(x = Erameans$mtemp[3], xend = Erameans$mtemp[3], y = 0, yend = 7, alpha = 0.2), color = "blue") +
  annotate(geom="text", x=150, y=6.5, label="<- Mean Tempi") +
  geom_rug() +
  theme_minimal() +
  ylim(0, 7) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  ) 
```

***
There is a lot to discover in this plot so let's start from the beginning. For the plot the limit on the y-axis has been set to 7, which cuts off 14 songs that have a much higher Standard Deviation (SD) that the other songs.
First of all, one can see the very distinct tempo means, each color corresponds with the color of the era. The means of the Rock Classics and the Rock Subgenres are close to eachother, while the mean tempo of Rock 'n Roll is higher than the other era's. This could very well be because of the fact that Rock Classics and Rock Subgenres have more styles of rock than Rock 'n Roll.

Next to that, one can see that the overall size and opacity of the Rock 'n Roll era seems to be lower than the other era's, this means that the duration and loudness of the era is mostly lower than other era's. There are some songs within the Rock Subgenres era that show similar features, but not as much as the Rock 'n Roll era.


### The era of Rock Classics, where 120 bpm was king
```{r}
RockNRoll <- get_playlist_audio_features("thesoundofspotify", "2ii0K7mNdB89XgcOCUM64t")  %>%
 slice(1:70)
RockClassics <- get_playlist_audio_features("thesoundofspotify", "37i9dQZF1DWXRqgorJj26U") %>%
 slice(1:70)
RockSubGenres <- get_playlist_audio_features("thesoundofspotify", "5dZcUWhn9DP8oOWVQVYLq4") %>%
 slice(1:70)

Rocktemp <- bind_rows(
 RockNRoll %>% mutate(era = "Rock 'n Roll (50s, 60s)"),
 RockClassics %>% mutate(era = "Rock Classics (70s, 80s)"),
 RockSubGenres %>% mutate(era = "Rock Subgenres (90s, 00s)")
)


Rocktemp %>%
 mutate(era = factor(era)) %>%
 ggplot(aes(x = tempo)) +
 geom_histogram(bins = 15) +
 facet_grid(~era)
```


***
In this plot, the era's are plotted using histograms to visualize the distribution of tempi used in the era. As can be seen, the Rock 'n Roll and the Rock Subgenres era are both pretty even in their distributions. The Rock Classics era however, shows a big spike around 120 bpm. This shows a very big preference, in the era of Queen, a king was needed and the musicians chose 120 bpm to be it.

The explanation for the lack of a peak in the Rock 'n Roll era could be that the era was diverse within how the song was set up. A Rock 'n Roll song could be high in tempo and high in "swing", whereas another song could be very slow in tempo because it was more of a blues-y song. For the Subgenres era, the same explanation can be used. Some subgenres are mainly high in tempo, where others are mainly low in tempo. The Classics era has somewhat a similar pattern. However, it seems that a lot of remembered music was around 120 bpm, which is not coincidentally a tempo humans seem to be very fond of.

### In what key is Green Day's Basket Case?

``` {r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

``` {r}
library(grid)
library(gridExtra)

basket_case <-
  get_tidy_audio_analysis("6L89mwZXSOwYl76YXfX13s") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

basket_case %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "cosine",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")


```

***
Here, we see a keygram of Basket Case by Green Day. One of the most listened to songs of the Rock Subgenres era. This song is very straight forward regarding the key of the song. However, this keygram does something interesting in the beginning of the song. There, it seems to think that the song is in another key, which is indicated by the dark blue line, and afterwards it switches to the right key (Eb major). This might have to do with the way the song starts. With only vocals and minimal chords. This makes it harder for the algorithm to deduct the key of the song. When the song starts, the algorithm finds the right key in which it stays throughout most of the song. At the end, the song fades out, in which the keygram has no idea anymore on what key the song is in.


### Classifying songs from different Rock era's (NEW)
```{r echo=FALSE, message=FALSE, results=FALSE, warnings=FALSE}
library(tidymodels)
library(ggdendro)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  


RockNRollS <- get_playlist_audio_features("", "2ii0K7mNdB89XgcOCUM64t") 
RockClassicsS <- get_playlist_audio_features("", "37i9dQZF1DWXRqgorJj26U") 
RockSubGenresS <- get_playlist_audio_features("", "5dZcUWhn9DP8oOWVQVYLq4")

RockSmall <- bind_rows(
  RockNRollS %>% mutate(era = "Rock 'n Roll (50s, 60s)") %>% slice_head(n = 20),
  RockClassicsS %>% mutate(era = "Rock Classics (70s, 80s)") %>% slice_head(n = 20),
  RockSubGenresS %>% mutate(era = "Rock Subgenres (90s, 00s)") %>% slice_head(n = 20)
)
  
rock_features <-
  RockSmall %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>%
  mutate(
    era = factor(era),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      ), names_repair = "unique"
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))

rock_recipe <-
  recipe(
    era ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = rock_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

rock_cv <- rock_features %>% vfold_cv(5)


knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
rock_knn <- 
  workflow() %>% 
  add_recipe(rock_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    rock_cv, 
    control = control_resamples(save_pred = TRUE)
  )


rock_knn %>% get_conf_mat() %>% autoplot(type = "heatmap") +
  xlab("Actual Era") +
  ylab("Predicted Era")


tree_model <-
  decision_tree() %>%
  set_mode("classification") %>% 
  set_engine("C5.0")
rock_tree <- 
  workflow() %>% 
  add_recipe(rock_recipe) %>% 
  add_model(tree_model) %>% 
  fit_resamples(
    rock_cv, 
    control = control_resamples(save_pred = TRUE)
  )


workflow() %>% 
  add_recipe(rock_recipe) %>% 
  add_model(tree_model) %>% 
  fit(rock_features) %>% 
  pluck("fit", "fit", "fit") %>%
  summary()

forest_model <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")
rock_forest <- 
  workflow() %>% 
  add_recipe(rock_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    rock_cv, 
    control = control_resamples(save_pred = TRUE)
  )


workflow() %>% 
  add_recipe(rock_recipe) %>% 
  add_model(forest_model) %>% 
  fit(rock_features) %>% 
  pluck("fit", "fit", "fit") %>%
  ranger::importance() %>% 
  enframe() %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")


rock_features %>%
  ggplot(aes(x = c02, y = duration, colour = era, size = acousticness)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Timbre Component 2",
    y = "Duration",
    size = "Acousticness",
    colour = "Era"
  )


```



Here you can see three plots that explain a k-nearest-neighbor classifier algorithm that was trained on 20 songs of every era. Firstly I would like to direct your attention to the barplot showing the importance of certain aspects of the song. Three stand out very distinctly, which are acousticness, duration and c06. Acousticness is the likelihood of the song being acoustic, duration is the actual duration of the song and c06 is an aspect of the timbre. 

These three are used in the scatterplot to visualize whether these do actually group the songs that the classifier was trained on. As can be seen in the plot, there are general areas where the era's group up. It seems that a big difference between the Rock subgenres and Rock 'n Roll arises in acousticness. Then the Rock classics seem to have a longer duration than both the other era's.

The other two plots show statistics about the classifier. It can be seen that the Rock Subgenres and Rock 'n Roll era get mixed only two times and that the Rock Classics era has some more errors. This would be expected, since this is somewhat of a transition period.



### The saddest Rock 'n Roll track in the corpus


``` {r}
Crying <-
  get_tidy_audio_analysis("6eLL7QTdMWdhhG4i3jHDR9") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

Crying %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()


Crying %>%
  compmus_self_similarity(pitches, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

***
In the two plots below, the biggest outlier of the corpus is represented with a cepstogram and a self similarity matrix (SSM) regarding pitches. The song is "Crying" by Roy Orbison. What can be extracted from both plots is that the song has a strong ending of some sort. In the cepstogram one sees a short rise in magnitude of the c02 vector, which means a certain aspect of the timbre is extra apparent at that moment. This is also seen within the SSM as two vertical and two horizontal lines, this means that near the end there is a bigger difference regarding the other parts of the song. When listening to the song, it becomes clear what this change is. Throughout the song, Orbison sings a sad song. However, in the end the singer reaches out for a strong high note. This is what causes the sudden change within the two plots.


 Appendix {.storyboard}
======================
### Rock 'n Roll (50s, 60s)

```{r echo=FALSE, message=FALSE}


rollplot <- RockNRoll %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
ggplot(
  aes(
    x = valence,
    y = danceability,
    color = mode,
    size = energy,
    label = track.name
    )
  ) +
  geom_point(alpha=0.4) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 1)) +
  theme(
    legend.position = "bottom"
  )

ggplotly(rollplot)
```






### Rock Classics era (70s, 80s)

``` {r}
classicplot <- RockClassics %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
ggplot(
  aes(
    x = valence,
    y = danceability,
    color = mode,
    size = energy,
    label = track.name
    )
  ) +
  geom_point(alpha=0.4) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 1)) +
  theme(
    legend.position = "bottom"
  )

ggplotly(classicplot)
```


### Rock Subgenres (90s, 00s)

``` {r}
subgplot <- RockSubGenres %>%
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
ggplot(
  aes(
    x = valence,
    y = danceability,
    color = mode,
    size = energy,
    label = track.name
    )
  ) +
  geom_point(alpha=0.4) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(limits = c(0, 1)) +
  theme(
    legend.position = "bottom"
  )

ggplotly(subgplot)
```


